{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup an experiment with given data and arch by adjusting config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import neptune\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import torch\n",
    "tnn = torch.nn\n",
    "top = torch.optim\n",
    "\n",
    "from torch.utils import data as tdt\n",
    "from src import utils\n",
    "%matplotlib inline\n",
    "\n",
    "ARCHS_DIR = 'archs'\n",
    "DATA_DIR = 'data'\n",
    "EXPERIMENTS_DIR = 'experiments'\n",
    "NEPTUNE_PRJ = 'indiacovidseva/covid-net'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA=\"cuda:0\"\n",
    "CPU=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(CUDA)\n",
    "    cd = torch.cuda.current_device()\n",
    "    print(\"Num devices:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", cd)\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(cd))\n",
    "    print(\"Device props:\", torch.cuda.get_device_properties(cd))\n",
    "#     print(torch.cuda.memory_summary(cd))\n",
    "else:\n",
    "    device = torch.device(CPU)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"NEPTUNE_ID\": \"\",\n",
    "    \"ID\": \"0000\",\n",
    "    \"DESC\": \"Check if notebook works.\",\n",
    "    \"ARCH\": \"v3\",\n",
    "    \"DATASET\": \"ds_cd_p_4020_owid_2020-07-14.csv.pt\",\n",
    "    \"IP_FEATURES\": [0],\n",
    "    \"OP_FEATURES\": [0],\n",
    "    \"AUX_FEATURES\": [0],\n",
    "    \"BATCH_SIZE\": 100,\n",
    "    \"HIDDEN_SIZE\": 20,\n",
    "    \"NUM_LAYERS\": 4,\n",
    "    \"DROPOUT\": 0.5,\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"NUM_EPOCHS\": 101\n",
    "}\n",
    "\n",
    "# checkpoint filename to resume training else \"\"\n",
    "RESUME_CP = \"\"\n",
    "\n",
    "# setup exp\n",
    "experiment_dir = EXPERIMENTS_DIR + '/' + config['ID']\n",
    "try:\n",
    "    os.mkdir(experiment_dir)\n",
    "except OSError:\n",
    "    print(\"!!WARNING!! EXPERIMENT ALREADY EXISTS:\", config['ID'])\n",
    "else:\n",
    "    print(\"Initialising experiment:\", config['ID'])\n",
    "print(\"Resume:\", RESUME_CP if RESUME_CP else False)\n",
    "\n",
    "# load data\n",
    "ds = torch.load(DATA_DIR + \"/\" + config['DATASET'])\n",
    "print(\"Dataset loaded\")\n",
    "config['DS'] = ds['config']\n",
    "print(config['DS'])\n",
    "\n",
    "# load arch\n",
    "arch_mod = importlib.import_module(\".\" + config['ARCH'], ARCHS_DIR)\n",
    "importlib.reload(arch_mod) # ensure changes are imported\n",
    "\n",
    "# init Net\n",
    "model = arch_mod.CovidNet(\n",
    "    ip_seq_len=config['DS']['IP_SEQ_LEN'], \n",
    "    op_seq_len=config['DS']['OP_SEQ_LEN'],\n",
    "    ip_size=len(config['IP_FEATURES']),\n",
    "    op_size=len(config['OP_FEATURES']),\n",
    "    hidden_size=config['HIDDEN_SIZE'], \n",
    "    num_layers=config['NUM_LAYERS'],\n",
    "    dropout=config['DROPOUT'],\n",
    "    ip_aux_size=len(config['AUX_FEATURES'])\n",
    ")\n",
    "model = model.to(device)\n",
    "print (\"Model initialised\")\n",
    "print(\"Num params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# init Loss and Optimizer\n",
    "loss_fn = tnn.L1Loss()\n",
    "optimizer = top.Adam(model.parameters(), lr=config['LEARNING_RATE'])\n",
    "\n",
    "# init dataset loaders\n",
    "trn_loader = tdt.DataLoader(ds['trn'], shuffle=True, batch_size=config['BATCH_SIZE'])\n",
    "val_loader = tdt.DataLoader(ds['val'], shuffle=True, batch_size=config['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_loss_vals = []\n",
    "val_loss_vals = []\n",
    "trn_acc_vals = []\n",
    "val_acc_vals = []\n",
    "e = 0\n",
    "min_val_loss = np.Inf\n",
    "max_val_acc = 0\n",
    "\n",
    "if RESUME_CP:\n",
    "    N = config['NUM_EPOCHS']\n",
    "    cp = utils.load_checkpoint(experiment_dir, RESUME_CP, device=device)\n",
    "    config, e, md, od = cp['config'], cp['epoch'], cp['model_state_dict'], cp['optimizer_state_dict']\n",
    "    trn_loss_vals, val_loss_vals, min_val_loss = cp['trn_losses'], cp['val_losses'], cp['min_val_loss']\n",
    "    trn_acc_vals, val_acc_vals, max_val_acc = cp['trn_acc'], cp['val_acc'], cp['max_val_acc']\n",
    "    e+=1\n",
    "    config['NUM_EPOCHS'] = N\n",
    "    model.load_state_dict(md)\n",
    "    optimizer.load_state_dict(od)\n",
    "\n",
    "# Neptune\n",
    "neptune_prj = neptune.init(NEPTUNE_PRJ)\n",
    "if config['NEPTUNE_ID']:\n",
    "    neptune_exp = neptune_prj.get_experiments(id=config['NEPTUNE_ID'])[0]\n",
    "else:\n",
    "    neptune_exp = neptune.create_experiment(name=config['ID'], params=config)\n",
    "    config['NEPTUNE_ID'] = neptune_exp.id\n",
    "\n",
    "# TRAIN\n",
    "print(\"BEGIN: [\", dt.datetime.now(), \"]\")\n",
    "while e < config['NUM_EPOCHS']:\n",
    "    model.train()\n",
    "    trn_losses = []\n",
    "    trn_ops = []\n",
    "    for data in trn_loader:\n",
    "        ip, ip_aux, op = data\n",
    "        ip = ip[:, :, config['IP_FEATURES']].to(device)\n",
    "        ip_aux = ip_aux[:, config['AUX_FEATURES']].to(device)\n",
    "        op = op[:, :, config['OP_FEATURES']].to(device)\n",
    "        optimizer.zero_grad() # set grads to 0\n",
    "        preds = model(\n",
    "            ip.view(-1, config['DS']['IP_SEQ_LEN'], len(config['IP_FEATURES'])),\n",
    "            ip_aux.view(-1, len(config['AUX_FEATURES']))\n",
    "        ) # predict\n",
    "        loss = loss_fn(preds, op.view(-1, config['DS']['OP_SEQ_LEN'], len(config['OP_FEATURES']))) # calc loss\n",
    "        loss.backward() # calc and assign grads\n",
    "        optimizer.step() # update weights\n",
    "        trn_losses.append(loss) # logging\n",
    "        trn_ops.append(op.mean())\n",
    "    avg_trn_loss = torch.stack(trn_losses).mean().item()\n",
    "    avg_trn_acc = 1 - avg_trn_loss / torch.stack(trn_ops).mean().item()\n",
    "    trn_loss_vals.append(avg_trn_loss * 10000)\n",
    "    trn_acc_vals.append(avg_trn_acc * 100)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        val_ops = []\n",
    "        for data in val_loader:\n",
    "            ip, ip_aux, op = data\n",
    "            ip = ip[:, :, config['IP_FEATURES']].to(device)\n",
    "            ip_aux = ip_aux[:, config['AUX_FEATURES']].to(device)\n",
    "            op = op[:, :, config['OP_FEATURES']].to(device)\n",
    "            preds = model(\n",
    "                ip.view(-1, config['DS']['IP_SEQ_LEN'], len(config['IP_FEATURES'])),\n",
    "                ip_aux.view(-1, len(config['AUX_FEATURES']))\n",
    "            )\n",
    "            loss = loss_fn(preds, op.view(-1, config['DS']['OP_SEQ_LEN'], len(config['OP_FEATURES'])))\n",
    "            val_losses.append(loss)\n",
    "            val_ops.append(op.mean())\n",
    "        avg_val_loss = torch.stack(val_losses).mean().item()\n",
    "        avg_val_acc = 1 - avg_val_loss / torch.stack(val_ops).mean().item()\n",
    "        val_loss_vals.append(avg_val_loss * 10000)\n",
    "        val_acc_vals.append(avg_val_acc * 100)\n",
    "    \n",
    "    neptune_exp.log_metric('validation accuracy', avg_val_acc*100)\n",
    "    neptune_exp.log_metric('training accuracy', avg_trn_acc*100)\n",
    "    neptune_exp.log_metric('validation loss', avg_val_loss*1e4)\n",
    "    neptune_exp.log_metric('training loss', avg_trn_loss*1e4)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\n",
    "            \"[\", dt.datetime.now(), \"] epoch:\", f\"{e:5}\", \n",
    "            \"val_acc:\", f\"{avg_val_acc*100: 4.2f}\", \"trn_acc:\", f\"{avg_trn_acc*100: 4.2f}\", \n",
    "            \"val_loss:\", f\"{avg_val_loss*1e4: 4.2f}\", \"trn_loss:\", f\"{avg_trn_loss*1e4: 4.2f}\"\n",
    "        )\n",
    "        if e%100==0:\n",
    "            utils.save_checkpoint(\n",
    "                config, e, model, optimizer, \n",
    "                trn_loss_vals, val_loss_vals, min_val_loss,\n",
    "                trn_acc_vals, val_acc_vals, max_val_acc,\n",
    "                experiment_dir, \"/latest-e\" + str(e) + \".pt\"\n",
    "            )\n",
    "        if avg_val_acc >= max_val_acc:\n",
    "            max_val_acc = avg_val_acc\n",
    "            utils.save_checkpoint(\n",
    "                config, e, model, optimizer, \n",
    "                trn_loss_vals, val_loss_vals, min_val_loss,\n",
    "                trn_acc_vals, val_acc_vals, max_val_acc,\n",
    "                experiment_dir, \"/best-e\" + str(e) + \".pt\"\n",
    "            )\n",
    "    e+=1\n",
    "print(\"END: [\", dt.datetime.now(), \"]\")\n",
    "\n",
    "neptune_exp.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss & acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loss = pd.DataFrame({\n",
    "    'trn_loss': trn_loss_vals,\n",
    "    'val_loss': val_loss_vals\n",
    "})\n",
    "\n",
    "df_acc = pd.DataFrame({\n",
    "    'trn_acc': trn_acc_vals,\n",
    "    'val_acc': val_acc_vals\n",
    "})\n",
    "\n",
    "# smoothen\n",
    "df_loss['trn_loss'] = df_loss['trn_loss'].rolling(3, min_periods=1, center=True).mean()\n",
    "df_loss['val_loss'] = df_loss['val_loss'].rolling(3, min_periods=1, center=True).mean()\n",
    "df_acc['trn_acc'] = df_acc['trn_acc'].rolling(3, min_periods=1, center=True).mean()\n",
    "df_acc['val_acc'] = df_acc['val_acc'].rolling(3, min_periods=1, center=True).mean()\n",
    "\n",
    "_ = df_loss[2:].plot(\n",
    "    y=['trn_loss', 'val_loss'],\n",
    "    title='Loss per epoch',\n",
    "    subplots=False,\n",
    "    figsize=(5,5),\n",
    "    sharex=False,\n",
    "    logy=True\n",
    ")\n",
    "_ = df_acc[2:].plot(\n",
    "    y=['trn_acc', 'val_acc'],\n",
    "    title='Acc per epoch',\n",
    "    subplots=False,\n",
    "    figsize=(5,5),\n",
    "    sharex=False,\n",
    "    logy=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "neptune": {
   "notebookId": "2a3f52ed-1c5c-40fb-8732-389e4eb67922"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
