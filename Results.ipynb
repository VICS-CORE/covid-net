{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate results in various formats from one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as rq\n",
    "import datetime as dt\n",
    "import torch\n",
    "import json\n",
    "import neptune\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.dates import DayLocator, AutoDateLocator, ConciseDateFormatter\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "EXPERIMENTS_DIR = 'experiments'\n",
    "DEVICE = 'cpu'\n",
    "NEPTUNE_PRJ = 'indiacovidseva/covid-net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = \"0001_test\"\n",
    "checkpoint = \"latest-e100.pt\"\n",
    "\n",
    "model, cp = utils.load_model(experiment_id, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['location', 'date', 'total_cases', 'new_cases', 'total_deaths', 'new_deaths', 'population']\n",
    "dates = ['date']\n",
    "df = pd.read_csv(DATA_DIR + \"/\" + cp['config']['DS']['SRC'],\n",
    "                 usecols=cols,\n",
    "                 parse_dates=dates)\n",
    "df = utils.fix_anomalies(df)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict from OWID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"India\"\n",
    "n_days_prediction = 200\n",
    "\n",
    "# restrict predictions if outputs != inputs\n",
    "if cp['config']['IP_FEATURES'] != cp['config']['OP_FEATURES']:\n",
    "    op_len = cp['config']['DS']['OP_SEQ_LEN']\n",
    "    print(\"WARNING: Input features and output features are different. Cannot predict more than\", op_len, \"days.\")\n",
    "    n_days_prediction = op_len\n",
    "\n",
    "IP_SEQ_LEN = cp['config']['DS']['IP_SEQ_LEN']\n",
    "OP_SEQ_LEN = cp['config']['DS']['OP_SEQ_LEN']\n",
    "pop_fct = df.loc[df.location==c, 'population'].iloc[0] / 1000\n",
    "test_data = np.array(df.loc[(df.location==c) & (df.total_cases>=100), cp['config']['DS']['FEATURES']].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "in_data = test_data[-IP_SEQ_LEN:, cp['config']['IP_FEATURES']]\n",
    "out_data = np.ndarray(shape=(0, len(cp['config']['OP_FEATURES'])), dtype=np.float32)\n",
    "for i in range(int(n_days_prediction / OP_SEQ_LEN)):\n",
    "    ip = torch.tensor(\n",
    "        in_data,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    ip = ip.to(DEVICE)\n",
    "    pred = model.predict(ip.view(1, IP_SEQ_LEN, len(cp['config']['IP_FEATURES']))).view(OP_SEQ_LEN, len(cp['config']['OP_FEATURES']))\n",
    "    in_data = np.append(in_data[-IP_SEQ_LEN+OP_SEQ_LEN:, :], pred.cpu().numpy(), axis=0)\n",
    "    out_data = np.append(out_data, pred.cpu().numpy(), axis=0)\n",
    "\n",
    "for o in cp['config']['IP_FEATURES']:\n",
    "    orig_df = pd.DataFrame({\n",
    "        'actual': test_data[:,o] * pop_fct\n",
    "    })\n",
    "    fut_df = pd.DataFrame({\n",
    "        'predicted': out_data[:,o] * pop_fct\n",
    "    })\n",
    "    # print(fut_df['predicted'].astype('int').to_csv(sep='|', index=False))\n",
    "    orig_df = orig_df.append(fut_df, ignore_index=True, sort=False)\n",
    "    orig_df['total'] = (orig_df['actual'].fillna(0) + orig_df['predicted'].fillna(0)).cumsum()\n",
    "\n",
    "    start_date = df.loc[(df.location==c) & (df.total_cases>=100)]['date'].iloc[0]\n",
    "    orig_df['Date'] = pd.Series([start_date + dt.timedelta(days=i) for i in range(len(orig_df))])\n",
    "    ax = orig_df.plot(\n",
    "        x='Date',\n",
    "        y=['actual', 'predicted'],\n",
    "        title=c + ' ' + cp['config']['DS']['FEATURES'][o],\n",
    "        figsize=(10,6),\n",
    "        grid=True\n",
    "    )\n",
    "    mn_l = DayLocator()\n",
    "    ax.xaxis.set_minor_locator(mn_l)\n",
    "    mj_l = AutoDateLocator()\n",
    "    mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "    ax.xaxis.set_major_formatter(mj_f)\n",
    "    # orig_df['total'] = orig_df['total'].astype('int')\n",
    "    # orig_df['predicted'] = orig_df['predicted'].fillna(0).astype('int')\n",
    "    # print(orig_df.tail(n_days_prediction))\n",
    "\n",
    "    # arrow\n",
    "    # peakx = 172\n",
    "    # peak = orig_df.iloc[peakx]\n",
    "    # peak_desc = peak['Date'].strftime(\"%d-%b\") + \"\\n\" + str(int(peak['predicted']))\n",
    "    # _ = ax.annotate(\n",
    "    #     peak_desc, \n",
    "    #     xy=(peak['Date'] - dt.timedelta(days=1), peak['predicted']),\n",
    "    #     xytext=(peak['Date'] - dt.timedelta(days=45), peak['predicted'] * .9),\n",
    "    #     arrowprops={},\n",
    "    #     bbox={'facecolor':'white'}\n",
    "    # )\n",
    "\n",
    "    # _ = ax.axvline(x=peak['Date'], linewidth=1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statewise predictions (covid19india)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=rq.get('https://api.covid19india.org/v3/min/timeseries.min.json')\n",
    "ts = r.json()\n",
    "\n",
    "data = []\n",
    "for state in ts:\n",
    "    for date in ts[state]:\n",
    "        ttl = ts[state][date]['total']\n",
    "        data.append((state, date, ttl.get('confirmed', 0), ttl.get('deceased', 0), ttl.get('recovered', 0), ttl.get('tested', 0)))\n",
    "\n",
    "states_df = pd.DataFrame(data, columns=['state', 'date', 'confirmed', 'deceased', 'recovered', 'tested'])\n",
    "states_df['date'] = pd.to_datetime(states_df['date'])\n",
    "first_case_date = states_df['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.populationu.com/india-population\n",
    "STT_INFO = {\n",
    "    'AN' : {\"name\": \"Andaman & Nicobar Islands\", \"popn\": 450000},\n",
    "    'AP' : {\"name\": \"Andhra Pradesh\", \"popn\": 54000000},\n",
    "    'AR' : {\"name\": \"Arunachal Pradesh\", \"popn\": 30000000},\n",
    "    'AS' : {\"name\": \"Asaam\", \"popn\": 35000000},\n",
    "    'BR' : {\"name\": \"Bihar\", \"popn\": 123000000},\n",
    "    'CH' : {\"name\": \"Chandigarh\", \"popn\": 1200000},\n",
    "    'CT' : {\"name\": \"Chhattisgarh\", \"popn\": 29000000},\n",
    "    'DL' : {\"name\": \"Delhi\", \"popn\": 19500000},\n",
    "    'DN' : {\"name\": \"Dadra & Nagar Haveli and Daman & Diu\", \"popn\": 700000},\n",
    "    'GA' : {\"name\": \"Goa\", \"popn\": 1580000},\n",
    "    'GJ' : {\"name\": \"Gujarat\", \"popn\": 65000000},\n",
    "    'HP' : {\"name\": \"Himachal Pradesh\", \"popn\": 7400000},\n",
    "    'HR' : {\"name\": \"Haryana\", \"popn\": 28000000},\n",
    "    'JH' : {\"name\": \"Jharkhand\", \"popn\": 38000000},\n",
    "    'JK' : {\"name\": \"Jammu & Kashmir\", \"popn\": 13600000},\n",
    "    'KA' : {\"name\": \"Karnataka\", \"popn\": 67000000},\n",
    "    'KL' : {\"name\": \"Kerala\", \"popn\": 36000000},\n",
    "    'LA' : {\"name\": \"Ladakh\", \"popn\": 325000},\n",
    "    'MH' : {\"name\": \"Maharashtra\", \"popn\": 122000000},\n",
    "    'ML' : {\"name\": \"Meghalaya\", \"popn\": 3400000},\n",
    "    'MN' : {\"name\": \"Manipur\", \"popn\": 3000000},\n",
    "    'MP' : {\"name\": \"Madhya Pradesh\", \"popn\": 84000000},\n",
    "    'MZ' : {\"name\": \"Mizoram\", \"popn\": 1200000},\n",
    "    'NL' : {\"name\": \"Nagaland\", \"popn\": 2200000},\n",
    "    'OR' : {\"name\": \"Odisha\", \"popn\": 46000000},\n",
    "    'PB' : {\"name\": \"Punjab\", \"popn\": 30000000},\n",
    "    'PY' : {\"name\": \"Puducherry\", \"popn\": 1500000},\n",
    "    'RJ' : {\"name\": \"Rajasthan\", \"popn\": 80000000},\n",
    "    'TG' : {\"name\": \"Telangana\", \"popn\": 39000000},\n",
    "    'TN' : {\"name\": \"Tamil Nadu\", \"popn\": 77000000},\n",
    "    'TR' : {\"name\": \"Tripura\", \"popn\": 4100000},\n",
    "    'UP' : {\"name\": \"Uttar Pradesh\", \"popn\": 235000000},\n",
    "    'UT' : {\"name\": \"Uttarakhand\", \"popn\": 11000000},\n",
    "    'WB' : {\"name\": \"West Bengal\", \"popn\": 98000000},\n",
    "#     'SK' : {\"name\": \"Sikkim\", \"popn\": 681000},\n",
    "#     'UN' : {\"name\": \"Unassigned\", \"popn\": 40000000}, #avg pop\n",
    "#     'LD' : {\"name\": \"Lakshadweep\", \"popn\": 75000}\n",
    "}\n",
    "\n",
    "# uncomment for India\n",
    "# STT_INFO = {\n",
    "#     'TT' : {\"name\": \"India\", \"popn\": 1387155000}\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy state data: fruit country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data for testing\n",
    "# SET 1 - 10 states\n",
    "# STT_INFO = {\n",
    "#     'A': {\"name\": \"Apple\", \"popn\": 10000000},\n",
    "#     'B': {\"name\": \"Berry\", \"popn\": 10000000},\n",
    "#     'C': {\"name\": \"Cherry\", \"popn\": 10000000},\n",
    "#     'D': {\"name\": \"Dates\", \"popn\": 10000000},\n",
    "#     'E': {\"name\": \"Elderberry\", \"popn\": 10000000},\n",
    "#     'F': {\"name\": \"Fig\", \"popn\": 10000000},\n",
    "#     'G': {\"name\": \"Grape\", \"popn\": 10000000},\n",
    "#     'H': {\"name\": \"Honeysuckle\", \"popn\": 10000000},\n",
    "#     'I': {\"name\": \"Icaco\", \"popn\": 10000000},\n",
    "#     'J': {\"name\": \"Jujube\", \"popn\": 10000000},\n",
    "# }\n",
    "# total = 100\n",
    "# SET 2 - 1 agg state\n",
    "STT_INFO = {\n",
    "    'Z': {\"name\": \"FruitCountry1000x\", \"popn\": 10000000},\n",
    "}\n",
    "total = 1000\n",
    "\n",
    "\n",
    "r = {\n",
    "    'state': [],\n",
    "    'date': [],\n",
    "    'total': []\n",
    "}\n",
    "\n",
    "start_date = dt.datetime(day=1, month=3, year=2020)\n",
    "end_date = dt.datetime.now()\n",
    "while start_date <= end_date:\n",
    "    for s in STT_INFO:\n",
    "        r['state'].append(s)\n",
    "        r['date'].append(start_date)\n",
    "        r['total'].append(total)\n",
    "    total *= 1.03\n",
    "    start_date += dt.timedelta(days=1)\n",
    "states_df = pd.DataFrame(r)\n",
    "states_df['date'] = pd.to_datetime(states_df['date'])\n",
    "states_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(df):\n",
    "    '''Fill missing dates in an irregular timeline'''\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    idx = pd.date_range(min_date, max_date)\n",
    "    \n",
    "    df.index = pd.DatetimeIndex(df.date)\n",
    "    df = df.drop(columns=['date'])\n",
    "    return df.reindex(idx, method='pad').reset_index().rename(columns={'index':'date'})\n",
    "\n",
    "def prefill(df, min_date):\n",
    "    '''Fill zeros from first_case_date to df.date.min()'''\n",
    "    assert(len(df.state.unique()) == 1)\n",
    "    s = df.state.unique().item()\n",
    "    min_date = min_date\n",
    "    max_date = df['date'].max()\n",
    "    idx = pd.date_range(min_date, max_date)\n",
    "    \n",
    "    df.index = pd.DatetimeIndex(df.date)\n",
    "    df = df.drop(columns=['date'])\n",
    "    return df.reindex(idx).reset_index().rename(columns={'index':'date'}).fillna({'state':s, 'total':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_SEQ_LEN = cp['config']['DS']['IP_SEQ_LEN']\n",
    "OP_SEQ_LEN = cp['config']['DS']['OP_SEQ_LEN']\n",
    "\n",
    "plot_feature = 0 # 0:confirmed, 1:deaths\n",
    "prediction_offset = 1 # how many days of data to skip\n",
    "n_days_prediction = 200 # number of days for prediction\n",
    "n_days_data = len(expand(states_df.loc[states_df['state']=='TT']))\n",
    "assert(n_days_prediction%OP_SEQ_LEN == 0)\n",
    "\n",
    "agg_days = n_days_data - prediction_offset + n_days_prediction # number of days for plotting agg curve i.e. prediction + actual data \n",
    "states_agg = np.zeros(agg_days)\n",
    "\n",
    "ax = None\n",
    "api = {}\n",
    "for state in STT_INFO:\n",
    "    pop_fct = STT_INFO[state][\"popn\"] / 1000\n",
    "    \n",
    "    state_df = states_df.loc[states_df['state']==state][:-prediction_offset] # skip todays data. covid19 returns incomplete.\n",
    "    state_df = prefill(expand(state_df), first_case_date)\n",
    "    state_df['new_cases'] = state_df['confirmed'] - state_df['confirmed'].shift(1).fillna(0)\n",
    "    state_df['new_deaths'] = state_df['deceased'] - state_df['deceased'].shift(1).fillna(0)\n",
    "    state_df['new_recovered'] = state_df['recovered'] - state_df['recovered'].shift(1).fillna(0)\n",
    "    state_df['new_tests'] = state_df['tested'] - state_df['tested'].shift(1).fillna(0)\n",
    "    test_data = np.array(state_df[cp['config']['DS']['FEATURES']].rolling(7, center=True, min_periods=1).mean() / pop_fct, dtype=np.float32)\n",
    "\n",
    "    in_data = test_data[-IP_SEQ_LEN:, cp['config']['IP_FEATURES']]\n",
    "    out_data = np.ndarray(shape=(0, len(cp['config']['OP_FEATURES'])), dtype=np.float32)\n",
    "    for i in range(int(n_days_prediction / OP_SEQ_LEN)):\n",
    "        ip = torch.tensor(\n",
    "            in_data,\n",
    "            dtype=torch.float32\n",
    "        ).to(DEVICE)\n",
    "        try:\n",
    "            pred = model.predict(ip.view(-1, IP_SEQ_LEN, len(cp['config']['IP_FEATURES']))).view(OP_SEQ_LEN, len(cp['config']['OP_FEATURES']))\n",
    "        except Exception as e:\n",
    "            print(state, e)\n",
    "        in_data = np.append(in_data[-IP_SEQ_LEN+OP_SEQ_LEN:, :], pred.cpu().numpy(), axis=0)\n",
    "        out_data = np.append(out_data, pred.cpu().numpy(), axis=0)\n",
    "    \n",
    "    sn = STT_INFO[state]['name']\n",
    "    orig_df = pd.DataFrame({\n",
    "        'actual': np.array(test_data[:,plot_feature] * pop_fct, dtype=np.int)\n",
    "    })\n",
    "    fut_df = pd.DataFrame({\n",
    "        'predicted': np.array(out_data[:,plot_feature] * pop_fct, dtype=np.int)\n",
    "    })\n",
    "    # print(fut_df.to_csv(sep='|'))\n",
    "    orig_df = orig_df.append(fut_df, ignore_index=True, sort=False)\n",
    "    orig_df[sn] = orig_df['actual'].fillna(0) + orig_df['predicted'].fillna(0)\n",
    "    orig_df['total'] = orig_df[sn].cumsum()\n",
    "    \n",
    "    states_agg += np.array(orig_df[sn][-agg_days:].fillna(0))\n",
    "\n",
    "    # generate date col for orig_df from state_df\n",
    "    start_date = state_df['date'].iloc[0]\n",
    "    orig_df['Date'] = pd.to_datetime([(start_date + dt.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(len(orig_df))])\n",
    "#     if orig_df[sn].max() < 10000: # or orig_df[sn].max() < 5000:\n",
    "#         continue\n",
    "    \n",
    "    # print state, cumulative, peak\n",
    "    peak = orig_df.loc[orig_df[sn].idxmax()]\n",
    "    print(sn, \"|\", peak['Date'].strftime(\"%b %d\"), \"|\", int(peak[sn]), \"|\", int(orig_df['total'].iloc[-1]))\n",
    "    \n",
    "    # export data for API\n",
    "    orig_df['daily_deaths'] = orig_df[sn] * 0.028\n",
    "    orig_df['daily_recovered'] = orig_df[sn].shift(14, fill_value=0) - orig_df['daily_deaths'].shift(7, fill_value=0)\n",
    "    orig_df['daily_active'] = orig_df[sn] - orig_df['daily_recovered'] - orig_df['daily_deaths']\n",
    "    \n",
    "    api[state] = {}\n",
    "    for idx, row in orig_df[-agg_days:].iterrows():\n",
    "        row_date = row['Date'].strftime(\"%Y-%m-%d\")\n",
    "        api[state][row_date] = {\n",
    "            \"delta\": {\n",
    "                \"confirmed\": int(row[sn]),\n",
    "                \"deceased\": int(row['daily_deaths']),\n",
    "                \"recovered\": int(row['daily_recovered']),\n",
    "                \"active\": int(row['daily_active'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    # plot state chart\n",
    "    ax = orig_df.plot(\n",
    "        x='Date',\n",
    "        y=[sn],\n",
    "        title='Daily Cases',\n",
    "        figsize=(15,10),\n",
    "        grid=True,\n",
    "        ax=ax,\n",
    "        lw=3\n",
    "    )\n",
    "    mn_l = DayLocator()\n",
    "    ax.xaxis.set_minor_locator(mn_l)\n",
    "    mj_l = AutoDateLocator()\n",
    "    mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "    ax.xaxis.set_major_formatter(mj_f)\n",
    "\n",
    "# plot aggregate chart\n",
    "cum_df = pd.DataFrame({\n",
    "    'states_agg': states_agg \n",
    "})\n",
    "last_date = orig_df['Date'].iloc[-1].to_pydatetime()\n",
    "start_date = last_date - dt.timedelta(days=agg_days)\n",
    "cum_df['Date'] = pd.to_datetime([(start_date + dt.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(len(cum_df))])\n",
    "ax = cum_df.plot(\n",
    "    x='Date',\n",
    "    y=['states_agg'],\n",
    "    title='Aggregate daily cases',\n",
    "    figsize=(15,10),\n",
    "    grid=True,\n",
    "    lw=3\n",
    ")\n",
    "mn_l = DayLocator()\n",
    "ax.xaxis.set_minor_locator(mn_l)\n",
    "mj_l = AutoDateLocator()\n",
    "mj_f = ConciseDateFormatter(mj_l, show_offset=False)\n",
    "ax.xaxis.set_major_formatter(mj_f)\n",
    "\n",
    "# plot peak in agg\n",
    "peakx = 178\n",
    "peak = cum_df.iloc[peakx]\n",
    "peak_desc = peak['Date'].strftime(\"%d-%b\") + \"\\n\" + str(int(peak['states_agg']))\n",
    "_ = ax.annotate(\n",
    "    peak_desc, \n",
    "    xy=(peak['Date'] + dt.timedelta(days=1), peak['states_agg']),\n",
    "    xytext=(peak['Date'] + dt.timedelta(days=45), peak['states_agg'] * .9),\n",
    "    arrowprops={},\n",
    "    bbox={'facecolor':'white'}\n",
    ")\n",
    "_ = ax.axvline(x=peak['Date'], linewidth=1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export JSON for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate predictions\n",
    "api['TT'] = {}\n",
    "for state in api:\n",
    "    if state == 'TT':\n",
    "        continue\n",
    "    for date in api[state]:\n",
    "        api['TT'][date] = api['TT'].get(date, {'delta':{}, 'total':{}})\n",
    "        for k in ['delta']: #'total'\n",
    "            api['TT'][date][k]['confirmed'] = api['TT'][date][k].get('confirmed', 0) + api[state][date][k]['confirmed']\n",
    "            api['TT'][date][k]['deceased'] = api['TT'][date][k].get('deceased', 0) + api[state][date][k]['deceased']\n",
    "            api['TT'][date][k]['recovered'] = api['TT'][date][k].get('recovered', 0) + api[state][date][k]['recovered']\n",
    "            api['TT'][date][k]['active'] = api['TT'][date][k].get('active', 0) + api[state][date][k]['active']\n",
    "\n",
    "# export\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    f.write(json.dumps(api, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export data for video player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate predictions\n",
    "api['TT'] = {}\n",
    "for state in api:\n",
    "    if state == 'TT':\n",
    "        continue\n",
    "    for date in api[state]:\n",
    "        api['TT'][date] = api['TT'].get(date, {})\n",
    "        api['TT'][date]['c'] = api['TT'][date].get('c', 0) + api[state][date]['delta']['confirmed']\n",
    "        api['TT'][date]['d'] = api['TT'][date].get('d', 0) + api[state][date]['delta']['deceased']\n",
    "        api['TT'][date]['r'] = api['TT'][date].get('r', 0) + api[state][date]['delta']['recovered']\n",
    "        api['TT'][date]['a'] = api['TT'][date].get('a', 0) + api[state][date]['delta']['active']\n",
    "\n",
    "# read previous and export\n",
    "k = (states_df.date.max().to_pydatetime() - dt.timedelta(days=prediction_offset)).strftime(\"%Y-%m-%d\")\n",
    "try:\n",
    "    with open(\"vp.json\", \"r\") as f:\n",
    "        out = json.loads(f.read())\n",
    "except Exception as e:\n",
    "    out = {}\n",
    "\n",
    "with open(\"vp.json\", \"w\") as f:\n",
    "    out[k] = {'TT': api['TT']}\n",
    "    f.write(json.dumps(out, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV export video player output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.DataFrame(out[k]['TT'])\n",
    "df_csv = df_csv.transpose()\n",
    "df_csv['c'].to_csv('vp_' + k + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload model to Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_prj = neptune.init(NEPTUNE_PRJ)\n",
    "neptune_exp = neptune_prj.get_experiments(id=cp['config']['NEPTUNE_ID'])[0]\n",
    "neptune_exp.log_artifact(EXPERIMENTS_DIR + \"/\" + experiment_id + \"/\" + checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
